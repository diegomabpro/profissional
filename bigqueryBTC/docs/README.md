# Projeto bigqueryBTC: Pipeline de ETL com BigQuery e AutomaÃ§Ã£o no GCP

Este projeto implementa um pipeline de ETL (Extract, Transform, Load) para processar dados histÃ³ricos de Bitcoin (BTC) da Yahoo Finance API, armazenÃ¡-los no Google Cloud Storage (GCS), transformÃ¡-los e carregÃ¡-los no BigQuery. O pipeline Ã© automatizado usando **Cloud Scheduler** para execuÃ§Ã£o periÃ³dica e **Pub/Sub** para notificaÃ§Ãµes de status.

---

## Estrutura do Projeto

ðŸ“‚ bigqueryBTC/
â”œâ”€â”€ ðŸ“‚ scripts/ # Scripts Python para ETL e automaÃ§Ã£o
â”‚ â”œâ”€â”€ extract.py # Extrai dados da Yahoo Finance API e salva no GCS
â”‚ â”œâ”€â”€ transform.py # Limpa e transforma os dados
â”‚ â”œâ”€â”€ load.py # Carrega os dados no BigQuery
â”‚ â”œâ”€â”€ etl_pipeline.py # Orquestra o pipeline de ETL
â”‚ â”œâ”€â”€ pubsub.py # Envia notificaÃ§Ãµes via Pub/Sub
â”‚ â”œâ”€â”€ scheduler.py # Configura o Cloud Scheduler
â”‚ â”œâ”€â”€ config.py # ConfiguraÃ§Ãµes globais do projeto
â”‚ â””â”€â”€ requirements.txt # DependÃªncias do projeto
â”œâ”€â”€ ðŸ“‚ sql/ # Scripts SQL para criaÃ§Ã£o das tabelas no BigQuery
â”‚ â”œâ”€â”€ create_raw.sql # Cria a tabela RAW
â”‚ â”œâ”€â”€ create_trusted.sql # Cria a tabela TRUSTED
â”‚ â”œâ”€â”€ create_refined.sql # Cria a tabela REFINED
â”œâ”€â”€ ðŸ“‚ docs/ # DocumentaÃ§Ã£o do projeto
â”‚ â”œâ”€â”€ README.md # Este arquivo
â”‚ â”œâ”€â”€ setup.md # Passos para configuraÃ§Ã£o no GCP
â”‚ â”œâ”€â”€ architecture.md # Arquitetura do pipeline
â”œâ”€â”€ ðŸ“‚ credentials/ # Pasta para armazenar credenciais
â”‚ â””â”€â”€ credenciais.json # Chave de serviÃ§o do GCP
â””â”€â”€ .gitignore # Ignora arquivos sensÃ­veis

---

## PrÃ©-requisitos

- **Conta no GCP**: Acesse o [Google Cloud Platform](https://cloud.google.com/) e crie um projeto.
- **APIs Ativadas**: Ative as APIs do GCS, BigQuery, Pub/Sub e Cloud Scheduler.
- **Python 3.8+**: Certifique-se de ter o Python instalado.
- **Chave de ServiÃ§o**: Gere uma chave de serviÃ§o no GCP e salve-a como `credenciais.json` na pasta `credentials/`.

---

## Como Executar

1. **Instale as DependÃªncias**:
   pip install -r scripts/requirements.txt

2. **Configure as Credenciais**:
   Defina a variÃ¡vel de ambiente:
      export GOOGLE_APPLICATION_CREDENTIALS="credentials/credenciais.json"

3. **Execute o pipeline**:
   Para execuÃ§Ã£o manual:
      python scripts/etl_pipeline.py

   Para execuÃ§Ã£o automÃ¡tica, configure o cloud scheduler:
      python scripts/scheduler.py

---

## DocumentaÃ§Ã£o Adicional
[ConfiguraÃ§Ã£o no GCP](setup.md): Passos detalhados para configurar o ambiente no GCP.

[Arquitetura do Pipeline](architecture.md): ExplicaÃ§Ã£o detalhada da arquitetura e fluxo de dados.

---